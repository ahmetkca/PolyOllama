# PolyOllama

Run multiple same or different open source large language models such as [Llama2](https://ollama.com/library/llama2), [Mistral](https://ollama.com/library/mistral) and [Gemma](https://ollama.com/library/gemma) in parallel simultaneously powered by [Ollama](https://ollama.com/). 

# Demo

https://github.com/ahmetkca/PolyOllama/assets/74574469/f0084d3c-6223-4f7e-9442-2aa5f79af10d

# Instructions to run it locally

> You need [Ollama](ollama.ai) installed on your computer.
> It has only been tested on macOS. It should run on Windows as well. (Please feel free to test it on Windows and create a issue if there is any)

cmd + k (to open the chat prompt)
alt + k (on Windows)

```bash
cd backend
bun install
bun run index.ts
```

```bash
cd frontend
bun install
bun run dev
```



> :warning: **Still work in progress**
